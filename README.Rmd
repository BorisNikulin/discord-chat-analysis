---
output: github_document
---

```{r global_options, include = F}
knitr::opts_chunk$set(warning = F, message = F, dev = 'svg', fig.align = 'center')
```
# Text Analysis of a Discord Chat Group

## Thanks to the Editor
A large thank you to [William Zhu](https://github.com/ZhuWilliam) for editing this poorly
written document into something nice.

## Data Acquisition
To get the data needed for analysis, there are two methods. First is the discord api's
[Get Channel Message](https://discordapp.com/developers/docs/resources/channel#get-channel-messages)
to manually retrieve, or a discord bot to do it for you.
However, if you do not wish to setup a bot, you can also use the second method, bare api calls in python.

Big thanks to
[DiscordArchiver](https://github.com/Jiiks/DiscordArchiver/blob/master/DiscordArchiver/Program.cs#L15)
for the undocumented (and probably old api that may be discontinued on October 16, 2017) url parameter for the token.

After creating `discord_chat_dl.py` and running it with the token, the channel id, and the id of the last message,
you can download all of the chat logs in a json format.

## Data Import
```{r import_json}
library(jsonlite)

chat_json <- read_json('discord_chat_anonymized.json')
```
This imports the json chat log as an R list.
However, the list is not uniform in fields across message entries as some messages have reactions, a feature introduced later in Discord's development that messages before the update do not have.
This inconsistency prevents running the list into `data.table::rbindlist`, so I used an alternative.
I extracted the relevant fields with
[purrr](https://cran.r-project.org/web/packages/purrr/vignettes/other-langs.html) and then stitched it back together into a
[data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html).
I then checked the result with [dplyr's](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) glimpse.

```{r import_tibble}
library(purrr)
library(lubridate)
library(data.table)
library(dplyr)

timestamps <- map(chat_json, ~.x$timestamp) %>% unlist() %>% ymd_hms()
usernames <- map(chat_json, ~.x$author$username) %>% unlist() %>% as.factor()
messages <- map(chat_json, ~.x$content) %>% unlist()

chat <- data.table(timestamp = timestamps, username = usernames, message = messages)

glimpse(chat)
```
## Data Analysis

### Word Tokenisation

To convert the `chat` data.table into a more convienient
[tidy format](http://tidytextmining.com/tidytext.html), with one token per row,
we can use [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html).
URLs, long digits, and common words can cause problems, but they can be filtered out
with regex and tidytext's `stop_words`.

```{r tokenization_word}
library(stringr)
library(tidytext)

chat[, message := str_replace(message, '(https?\\S+)|(d{4,})', '')]

words <- chat %>%
	unnest_tokens(word, message) %>%
	.[!data.table(stop_words), on = 'word', .(timestamp, username, word)] %>% # anti join
	.[, .N, .(username, word)] %>%
	setorder(-N)

glimpse(words)
words[, head(.SD, 3), username]
```

### Bigram Tokenisation

```{r tokenization_bigram}

```
