---
output: github_document
---

```{r global_options, include = F}
knitr::opts_chunk$set(warning = F, message = F, dev = 'png')
```
# Text Analysis of a Discord Chat Group

## Thanks to the Editor
A large thank you to [William Zhu](https://github.com/ZhuWilliam) for editing this poorly
written document into something nice.

## Data Acquisition
To get the data needed for analysis, one can go directly to the source via
[Get Channel Message](https://discordapp.com/developers/docs/resources/channel#get-channel-messages)
or use an existing tool to do it for you.
The most prominent utility is
[DiscordChatExporter](https://github.com/Tyrrrz/DiscordChatExporter).
Although the JSON schema it exports is slightly different to the Discord API spec,
not counting added data by the tool,
the data is easy to aquire and use.

Using DiscordChatExporter, an auth token, and a channel id
one can export all chat history in JSON format
and then import it into R.
Once imported, we can toss the extra data added about the channel export
by directly acessing the nested list of messages.

## Data Import
```{r import_json}
library(jsonlite)

# regular read of direct JSON file
#chat_json <- read_json('./discord_chat_anonymized.json')$messages

# in memory gunzip, highly recomended to compress large JSON logs
# read_json(...) == fromJson(..., simplifyVector = FALSE)
# but handles more than just a string filepath such as connections (R file objects)
log_gzipped <- gzfile('./discord_chat_anonymized.json.gz', open = 'rb') # fromJSON needs binary
chat_json <- fromJSON(log_gzipped, simplifyVector = FALSE)$messages
close(log_gzipped)
```

This imports the json messages as an R list.
However, the list is not uniform in fields across message entries as some messages have reactions,
a feature introduced later in Discord's development that messages before the update do not have.
This inconsistency prevents running the list into `data.table::rbindlist`, so I used an alternative method.
I extracted the relevant fields with
[purrr](https://cran.r-project.org/web/packages/purrr/vignettes/other-langs.html) and then stitched it back together into a
[data.table](https://cran.r-project.org/web/packages/data.table/vignettcs/datatable-intro.html).
I then checked the result with [dplyr's](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) glimpse.
Note that time was converted to PST/PDT.
All times shown will be with repsect to PST/PDT.

```{r import_data.table}
library(purrr)
library(data.table) # masks some of lubridate's functions
library(lubridate)
library(dplyr)
library(dtplyr)

chat <- data.table(
	timestamp = map_chr(chat_json, 'timestamp') %>% ymd_hms(),
	# Discord author JSON spec calls the field `username` not `name`
	username = map_chr(chat_json, c('author', 'name')) %>% as.factor(),
	message = map_chr(chat_json, 'content')
)
rm(chat_json)

chat[, timestamp := with_tz(timestamp, tzone = 'US/Pacific')] # convert to PST/PDT (same time)

glimpse(chat)
chat[1, timestamp]
```
## Data Tidying

### User and Message Tidying

Some users did not post much so we filter them out.
On top of filtering users, we remove URLs and long digits with regex.

```{r chat_tidying}
library(magrittr) # for %<>% (originator of %>%)
library(stringr)

chat %<>%
	.[!.[, .N, username][N < 100], on = 'username'] %>% # anti join
	.[, username := droplevels(username)] %>%
	.[, message := str_replace(message, '(https?\\S+)|(\\d{4,})', '')]
```

### Word Tokenization

To convert the `chat` data.table into a more convienient
[tidy format](http://tidytextmining.com/tidytext.html), with one token per row,
we can use [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html).
Common words can cause problems, but they can be removed with the help of tidytext's `stop_words`.

```{r tokenization_word}
library(tidytext)

words <- chat %>%
	unnest_tokens(word, message) %>%
	.[!data.table(stop_words), on = 'word'] # anti join

glimpse(words)
```

### Bigram Tokenization

Next, we prepare the tokenization of bigrams in much the same fashion as for words by using `unnest_tokens`, followed by
[tidyr's](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)
separate, and finally removing common words.

```{r tokenization_bigram}
library(tidyr)

bigrams <- chat %>%
	unnest_tokens(bigram, message, token = 'ngrams', n = 2) %>%
	data.table() %>% # unnest tokens on words returns the same structure but on bigrams it returns a tibble
	separate(bigram, c('word1', 'word2'), sep = ' ') %>%
	.[!word1 %in% stop_words$word & !word2 %in% stop_words$word,]

glimpse(bigrams)
```
## Data Analysis

### Word Counts

What are the top words?
Here we check the top three words per user.

```{r analysis_word_counts}
word_counts <- words %>%
	.[, .N, .(username, word)] %>%
	setorder(-N)

word_counts[, head(.SD, 3), username]
```

How about visually?

```{r analysis_word_counts_graphed}
library(ggplot2)

#word_counts %>%
	#.[head(setorder(.[, .(total = sum(N)), word], -total), 20), on = 'word'] %>%
	#ggplot(aes(reorder(word, total), N, fill = username)) +
	#geom_col() +
	#coord_flip()

word_counts %>%
	.[.[, .(total = sum(N)), word] %>%
		setorder(-total) %>%
		head(20)
	, on = 'word'] %>%
	ggplot(aes(reorder(word, total), N, fill = username)) +
	geom_col() +
	coord_flip() +
	labs(x = 'Word', y = 'Word Count', fill = 'Username')
```

<details>
<summary>
	WIP: Time Bases Analysis
</summary>
```{r analysis_weekly_chat_rate}
#words_by_day <- words %>%
    #.[, .(timestamp = floor_date(timestamp, 'day'))] %>%
    #.[, .(words_in_day = .N), timestamp] %>%
    #.[, .(timestamp, words_in_day, day_of_week = wday(timestamp, label = TRUE))]

words_by_day <- words %>%
	#.[, `:=`(username = NULL, word = NULL)] %T>% glimpse() %>%
	.[, .(timestamp = floor_date(timestamp, 'day'))] %>%
	.[, .(words_in_day = .N), timestamp] %>%
	.[, day_of_week := wday(timestamp, label = TRUE)]

plot <- ggplot(words_by_day, aes(timestamp, words_in_day)) +
	geom_line() +
	geom_smooth() +
	labs(x = 'Day', y = 'Non Stop Word Count in Day')

plot

theme_x_axis_text_45 <- theme(axis.text.x = element_text(angle = 45, vjust = 1.1,  hjust = 1.1))

plot +
	facet_grid(.~day_of_week) +
	theme_x_axis_text_45
	#theme(axis.text.x = element_text(angle = 90, vjust = 0.5,  hjust = 1))
```

```{r analysis_weekly_chat_rate_per_user}
words_by_day_per_user <- copy(words) %>%
	.[, `:=`(word = NULL, timestamp = floor_date(timestamp, 'day'))] %>%
	.[, .(words_in_day_per_user = .N), .(timestamp, username)] %>%
	.[, day_of_week := wday(timestamp, label = TRUE)]

plot <- ggplot(words_by_day_per_user, aes(timestamp, words_in_day_per_user)) +
	geom_line() +
	geom_smooth() +
	labs(x = 'Day', y = 'Non Stop Word Count In Day')

plot +
	facet_grid(username~.)

plot +
	facet_grid(username ~ day_of_week) +
	theme_x_axis_text_45
	#theme(axis.text.x = element_text(angle = 45, vjust = 1.1,  hjust = 1.1))
```

```{r analysis_hourly_chat_rate}
#TODO: check timezone and consider making the 4 in 4 hours per chunk a variable
chat[1, timestamp]
words_by_hour <- words %>%
	#.[, `:=`(username = NULL, word = NULL)] %T>% glimpse() %>%
	.[, .(timestamp = floor_date(timestamp, '4 hours'))] %>%
	.[, .(words_in_hour = .N), timestamp] %>%
	.[, hours_chunk := hour(timestamp)]

glimpse(words_by_hour)

words_by_hour[, head(.SD, 3), hours_chunk]
words_by_hour[, .(.N), hours_chunk]
copy(words)[, time := floor_date(timestamp, '4 hours')] %>%
	.[, hours_chunk := hour(time)] %>%
	.[hours_chunk == 1] %T>%
	.[, timestamp]

unique(tz(chat[, timestamp]))

hour_chunk_labeller <- function(hours_in_chunk)
{
	function(facet_var)
	{
		paste(facet_var, as.numeric(facet_var) + hours_in_chunk, sep = '-')
	}
}

ggplot(words_by_hour, aes(timestamp, words_in_hour)) +
	geom_line() +
	geom_smooth() +
	facet_grid(.~hours_chunk, labeller = as_labeller(hour_chunk_labeller(4))) +
	labs(x = 'Four Hour Segments', y = 'Non Stop Word Count In Four Hour Chunk') +
	theme_x_axis_text_45

words_by_hour_per_user <- copy(words) %>%
	.[, `:=`(word = NULL, timestamp = floor_date(timestamp, '4 hours'))] %>%
	.[, .(words_in_hour_per_user = .N), .(timestamp, username)] %>%
	.[, hours_chunk := hour(timestamp)]

glimpse(words_by_hour_per_user)

ggplot(words_by_hour_per_user, aes(timestamp, words_in_hour_per_user)) +
	geom_line() +
	geom_smooth() +
	facet_grid(username ~ hours_chunk,
			   labeller = labeller(hours_chunk = as_labeller(hour_chunk_labeller(4)))) +
	labs(x = 'Four Hour Segments', y = 'Non Stop Word Count In Four Hour Chunk') +
	theme_x_axis_text_45
```
</details>

### Bigram Counts

```{r analysis_bigram_counts}
bigram_counts_per_user <- bigrams %>%
	na.omit() %>%
	.[, .N, .(username, word1, word2)] %>%
	setorder(-N)

bigram_counts <- bigram_counts_per_user %>%
	.[, .(N = sum(N)), .(word1, word2)]

head(bigram_counts, 5)
bigram_counts_per_user[, head(.SD, 3), username]
```

### Characteristic Words

To answer the question of what words are characteristic of document among a corpus
we can use tf-idf.
In our case,
we consider the corpus to be all of the text in the logs
and each user's text as a document.
Now we can ask the question of what words differentiate each user.

tf-idf is a masuere that penalizes frequest words across documents, users in our case, (idf)
and rewards frequent words within a document, a user's text, (tf).
idf is the inverse document frequency which is low for words common in all documents
and tf is the term frequency or the relative frequency of a word within a document
which is high for common words within a document.
When the term frequency is multiplied with the inverse document frequency,
we get tf-idf which allows us to see the most common words of a user that are less common among all users
to see which words characterize users.

```{r analysis_tf_idf, fig.height = 10}
word_tf_idf <- word_counts %>%
	bind_tf_idf(word, username, N) %>%
	setorder(-tf_idf)

word_tf_idf[, head(.SD, 2), username]

# order within factors is wack but is correct as top tf-idf per user
word_tf_idf %>%
	.[, head(.SD, 6), username] %>%
	.[, word := factor(word, levels = rev(unique(word)))] %>%
	ggplot(aes(word, tf_idf)) +
	geom_col() +
	facet_wrap(~username, ncol = 2, scales = 'free') +
	coord_flip() +
	labs(title = 'Most Charateristics Words per User', x = NULL, y = 'tf-idf')
```

### Word Relationships

One way to see relationships between words is to consider consequitve words or n-grams.
Here were consider bigrams or pairs of consequitve words.
Plotting the network of bigrams allows us to see a Markov chain like structure
of the words said.

```{r analysis_relationship_bigram}
library(igraph)
library(ggraph)

set.seed(23)
bigram_counts %>%
	head(75) %>%
	graph_from_data_frame() %>%
	# doesnt get rid of the loops... one can hope
	simplify(remove.multiple = FALSE, remove.loops = TRUE) %>%
	ggraph(layout = 'fr') +
	geom_edge_link(
		aes(edge_alpha = N),
		show.legend = FALSE,
		arrow = grid::arrow(type = 'closed', length = unit(0.15, 'inches')),
		end_cap= circle(0.05, 'inches')
	) +
	geom_node_point(color = 'lightblue', size = 5) +
	geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
	labs(title = 'Most Frequent Bigrams')
```

```{r analysis_relationship_bigram_per_user_as_edges, fig.width = 8, fig.height = 10}
set.seed(23)
bigram_counts_per_user %>%
	.[, head(.SD, 3), username] %>%
	.[, .(word1, word2, username, N)] %T>%
	glimpse() %>%
	graph_from_data_frame() %T>%
	print() %>%
	simplify(remove.multiple = FALSE, remove.loops = TRUE) %>%
	ggraph(layout = 'fr') +
	geom_edge_link(
		arrow = grid::arrow(type = 'closed', length = unit(0.1, 'inches')),
		end_cap= circle(0.05, 'inches')
	) +
	geom_node_point(color = 'lightblue', size = 2) +
	geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
	facet_edges(~username) +
	labs(title = 'Most Frequent Bigrams by User as Edges') +
	coord_cartesian(xlim = c(16,29), expand = TRUE)
```
