---
output: github_document
---

```{r global_options, include = F}
knitr::opts_chunk$set(warning = F, message = F, dev = 'svg', fig.align = 'center')
```
# Text Analysis of a Discord Chat Group

## Thanks to the Editor
A large thank you to [William Zhu](https://github.com/ZhuWilliam) for editing this poorly
written document into something nice.

## Data Acquisition
To get the data needed for analysis, there are two methods. First is the discord api's
[Get Channel Message](https://discordapp.com/developers/docs/resources/channel#get-channel-messages)
to manually retrieve the messages. The second, is to get a discord bot to do it for you.
However, if you do not wish to setup a bot, you can use the first method to do bare api calls in python.

Big thanks to
[DiscordArchiver](https://github.com/Jiiks/DiscordArchiver/blob/master/DiscordArchiver/Program.cs#L15)
for the undocumented (and probably old api that may be discontinued on October 16, 2017) url parameter for the token.

After creating `discord_chat_dl.py` and running it with the token, the channel id, and the id of the last message,
you can download all of the chat logs in a json format.

## Data Import
```{r import_json}
library(jsonlite)

chat_json <- read_json('discord_chat_anonymized.json')
```
This imports the json chat log as an R list.
However, the list is not uniform in fields across message entries as some messages have reactions,
a feature introduced later in Discord's development that messages before the update do not have.
This inconsistency prevents running the list into `data.table::rbindlist`, so I used an alternative method.
I extracted the relevant fields with
[purrr](https://cran.r-project.org/web/packages/purrr/vignettes/other-langs.html) and then stitched it back together into a
[data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html).
I then checked the result with [dplyr's](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) glimpse.

```{r import_data.table}
library(purrr)
library(data.table) # masks some of lubridate's functions
library(lubridate)
library(dplyr)
library(dtplyr)

timestamps <- map(chat_json, ~.x$timestamp) %>% unlist() %>% ymd_hms()
usernames <- map(chat_json, ~.x$author$username) %>% unlist() %>% as.factor()
messages <- map(chat_json, ~.x$content) %>% unlist()

chat <- data.table(timestamp = timestamps, username = usernames, message = messages)

glimpse(chat)
```
## Data Tidying

### User and Message Tidying

Some users did not post much so we filter them out.
On top of filtering users, we remove URLs and long digits with regex.

```{r chat_tidying}
library(magrittr) # for %<>% (originator of %>%)
library(stringr)

chat %<>%
	.[!.[, .N, username][N < 100], on = 'username'] %>% # anti join
	.[, username := droplevels(username)] %>%
	.[, message := str_replace(message, '(https?\\S+)|(\\d{4,})', '')]
```

### Word Tokenization

To convert the `chat` data.table into a more convienient
[tidy format](http://tidytextmining.com/tidytext.html), with one token per row,
we can use [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html).
Common words can cause problems, but they can be removed with the help of tidytext's `stop_words`.

```{r tokenization_word}
library(tidytext)

words <- chat %>%
	unnest_tokens(word, message) %>%
	.[!data.table(stop_words), on = 'word'] # anti join

glimpse(words)
```

### Bigram Tokenization

Next, we prepare the tokenization of bigrams in much the same fashion as for words by using `unnest_tokens`, followed by
[tidyr's](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)
separate, and finally removing common words.

```{r tokenization_bigram}
library(tidyr)

bigrams <- chat %>%
	unnest_tokens(bigram, message, token = 'ngrams', n = 2) %>%
	data.table() %>% # unnest tokens on words returns the same structure but on bigrams it returns a tibble
	separate(bigram, c('word1', 'word2'), sep = ' ') %>%
	.[!word1 %in% stop_words$word & !word2 %in% stop_words$word,]

glimpse(bigrams)
```
## Data Analysis

### Word Counts

What are the top words?
Here we check the top three words per user.

```{r analysis_word_counts}
word_counts <- words %>%
	.[, .N, .(username, word)] %>%
	setorder(-N)

word_counts[, head(.SD, 3), username]
```

How about visually?

```{r analysis_word_counts_graphed}
library(ggplot2)

#word_counts %>%
	#.[head(setorder(.[, .(total = sum(N)), word], -total), 20), on = 'word'] %>%
	#ggplot(aes(reorder(word, total), N, fill = username)) +
	#geom_col() +
	#coord_flip()

word_counts %>%
	.[.[, .(total = sum(N)), word] %>%
		setorder(-total) %>%
		head(20)
	, on = 'word'] %>%
	ggplot(aes(reorder(word, total), N, fill = username)) +
	geom_col() +
	coord_flip() +
	labs(x = 'Word', y = 'Word Count', fill = 'Username')
```

```{r analysis_weekly_chat_rate}
#words_by_day <- words %>%
    #.[, .(timestamp = floor_date(timestamp, 'day'))] %>%
    #.[, .(words_in_day = .N), timestamp] %>%
    #.[, .(timestamp, words_in_day, day_of_week = wday(timestamp, label = TRUE))]

words_by_day <- words %>%
	#.[, `:=`(username = NULL, word = NULL)] %T>% glimpse() %>%
	.[, .(timestamp = floor_date(timestamp, 'day'))] %>%
	.[, timestamp := floor_date(timestamp, 'day')]%>%
	.[, .(words_in_day = .N), timestamp] %>%
	.[, day_of_week := wday(timestamp, label = TRUE)]

plot <- ggplot(words_by_day, aes(timestamp, words_in_day)) +
	geom_line() +
	geom_smooth() +
	labs(x = 'Day', y = 'Non Stop Word Count in Day')

plot

plot +
	facet_grid(.~day_of_week) +
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5,  hjust = 1))
```

```{r analysis_weekly_chat_rate_per_user}
words_by_day_per_user <- copy(words) %>%
	.[, `:=`(word = NULL, timestamp = floor_date(timestamp, 'day'))] %>%
	.[, .(words_in_day_per_user = .N), .(timestamp, username)] %>%
	.[, day_of_week := wday(timestamp, label = TRUE)]

plot <- ggplot(words_by_day_per_user, aes(timestamp, words_in_day_per_user)) +
	geom_line() +
	geom_smooth() +
	labs(x = 'Day', y = 'Non Stop Word Count In Day')

plot +
	facet_grid(username~.)

plot +
	facet_grid(username ~ day_of_week) +
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5,  hjust = 1))
```

### Bigram Counts

```{r analysis_bigram_counts}
bigram_counts <- bigrams %>%
	.[, .N, .(username, word1, word2)] %>%
	setorder(-N)

bigram_counts[, head(.SD, 3), username]
```
